{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import config.config as config\n",
    "from validation import Validation\n",
    "from Environment import Env\n",
    "from Agent_EKF import *\n",
    "from pathlib import Path\n",
    "\n",
    "import sys; sys.path.append('../analysis/')\n",
    "from my_utils import reset_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(datapath, seed_number, task='gain',\n",
    "             TOTAL_EPISODE=1e5, pro_noise=0.2, obs_noise=0.1, extensive=False):\n",
    "    if task == 'gain':\n",
    "        arg = config.ConfigGain(datapath)\n",
    "    elif task == 'gain_control':\n",
    "        arg = config.ConfigGainControl(datapath, pro_noise=pro_noise, obs_noise=obs_noise)\n",
    "    else:\n",
    "        raise ValueError('No such a task!')\n",
    "        \n",
    "    arg.SEED_NUMBER = seed_number\n",
    "    arg.save()\n",
    "    \n",
    "    # reproducibility\n",
    "    reset_seeds(arg.SEED_NUMBER)\n",
    "    \n",
    "    # initialize environment and agent\n",
    "    env = Env(arg)\n",
    "    agent = Agent(arg)\n",
    "    validator = Validation(arg.task, agent_name='EKF', extensive=extensive)\n",
    "    \n",
    "    # define exploration noise\n",
    "    init_expnoise_std = 0.8\n",
    "    noise = ActionNoise(arg.ACTION_DIM, mean=0, std=init_expnoise_std)\n",
    "\n",
    "    # Remove observation noise in the beginning to help learning in the early stage.\n",
    "    agent.bstep.obs_noise_range = None\n",
    "    \n",
    "    # Loop now\n",
    "    tot_t = 0\n",
    "    episode = agent.initial_episode\n",
    "    reward_log = []\n",
    "    rewarded_trial_log = []\n",
    "    step_log = []\n",
    "    actor_loss_log = 0\n",
    "    critic_loss_log = 0\n",
    "    num_update = 1e-5\n",
    "    dist_log = []\n",
    "    \n",
    "    LOG_FREQ = 100\n",
    "    VALIDATION_FREQ = 500\n",
    "    decrease_lr = True\n",
    "    REPLAY_PERIOD = 4    # critic update frequency\n",
    "    PRE_LEARN_PERIOD = arg.EKF_BATCH_SIZE * 50     # no learning in the first n trials\n",
    "\n",
    "    enable_mirror_traj = True     # store mirror image of each trajectory\n",
    "    pre_phase=True                # training phase I\n",
    "    \n",
    "    # Start loop\n",
    "    while episode < TOTAL_EPISODE:\n",
    "        # initialize a trial\n",
    "        cross_start_threshold = False\n",
    "        done = torch.zeros((1, 1), device=arg.device)\n",
    "        reward = torch.zeros((1, 1), device=arg.device)\n",
    "        \n",
    "        x = env.reset()\n",
    "        b, state = agent.bstep.reset(env.pro_gains, env.pro_noise_std, env.target_position)\n",
    "        state = state.to(arg.device)                      # the EKF state\n",
    "        last_action_raw = torch.zeros(1, arg.ACTION_DIM)  # the exploration noise-free action\n",
    "\n",
    "        for t in range(arg.EPISODE_LEN):\n",
    "            # 1. Check if the agent's action crosses the start threshold\n",
    "            if not cross_start_threshold and (last_action_raw.abs() > arg.TERMINAL_ACTION).any():\n",
    "                cross_start_threshold = True\n",
    "\n",
    "            # 2. Take an action based on current state and previous hidden states of RNN units\n",
    "            action, action_raw = agent.select_action(state, action_noise=noise)\n",
    "\n",
    "            # 3. Update the environment given the agent's action\n",
    "            next_x, reached_target, relative_dist = env(x, action, t)\n",
    "\n",
    "            # 4. Collect new observation and construct the next state\n",
    "            next_ox = agent.bstep.observation(next_x)\n",
    "            next_b = agent.bstep(b, next_ox, action, env.perturbation_vt, env.perturbation_wt)\n",
    "            next_state = agent.bstep.b_reshape(next_b).to(arg.device)\n",
    "\n",
    "            # 5. Check if agent stops\n",
    "            time_end = (t == arg.EPISODE_LEN - 1)\n",
    "            is_stop = env.is_stop(x, action)\n",
    "\n",
    "            # 6. Give reward if agent stopped       \n",
    "            if (is_stop and cross_start_threshold) or time_end:\n",
    "                done = torch.ones((1, 1), device=arg.device)\n",
    "                if is_stop and cross_start_threshold:\n",
    "                    reward = env.return_reward(x, reward_mode='mixed').view(-1, 1).to(arg.device)\n",
    "\n",
    "            # 7. Push transition into replay buffer\n",
    "            agent.memory.push(state, action.to(arg.device), next_state, reward, done)\n",
    "            # store mirrored transition reflected along the y-axis\n",
    "            if enable_mirror_traj and noise.std != init_expnoise_std:\n",
    "                agent.memory.push(*agent.mirror_traj(state, action, next_state), reward, done)      \n",
    "\n",
    "            # 8. Update timestep\n",
    "            last_action_raw = action_raw\n",
    "            state = next_state\n",
    "            x = next_x\n",
    "            b = next_b \n",
    "            tot_t += 1\n",
    "\n",
    "            # 9. Update model\n",
    "            if len(agent.memory.memory) > PRE_LEARN_PERIOD and tot_t % REPLAY_PERIOD == 0:\n",
    "                actor_loss, critic_loss = agent.learn()\n",
    "                actor_loss_log += actor_loss\n",
    "                critic_loss_log += critic_loss\n",
    "                num_update += 1\n",
    "\n",
    "            # 10. Trial ends if agent stops\n",
    "            if is_stop and cross_start_threshold:\n",
    "                break\n",
    "\n",
    "\n",
    "        # End of a trial\n",
    "        # Logs     \n",
    "        reward_log.append(reward.item())\n",
    "        rewarded_trial_log.append(int(reached_target & is_stop))\n",
    "        step_log.append(t + 1)\n",
    "        dist_log.append(relative_dist.item())\n",
    "\n",
    "        if episode % LOG_FREQ == LOG_FREQ - 1:\n",
    "            print(f\"t: {tot_t}, Ep: {episode}, action std: {noise.std:0.2f}\")\n",
    "            print(f\"mean steps: {np.mean(step_log):0.3f}, \"\n",
    "                  f\"mean reward: {np.mean(reward_log):0.3f}, \"\n",
    "                  f\"rewarded fraction: {np.mean(rewarded_trial_log):0.3f}, \"\n",
    "                  f\"relative distance: {np.mean(dist_log) * arg.LINEAR_SCALE:0.3f}, \"\n",
    "                  f\"obs noise: {agent.bstep.obs_noise_range}, \"\n",
    "                  f\"critic loss: {critic_loss_log / num_update:0.3f}, \"\n",
    "                  f\"actor loss: {-actor_loss_log / (num_update/2):0.3f}\")\n",
    "\n",
    "            # training phase III\n",
    "            if decrease_lr and (validator.data.reward_fraction > 0.8).any():\n",
    "                noise.reset(mean=0, std=0.4)\n",
    "                agent.actor_optim.param_groups[0]['lr'] = arg.decayed_lr\n",
    "                agent.critic_optim.param_groups[0]['lr'] = arg.decayed_lr\n",
    "                decrease_lr = False\n",
    "                print('Noise and learning rate are changed.')\n",
    "\n",
    "            # training phase II\n",
    "            if noise.std == init_expnoise_std and np.mean(rewarded_trial_log) > 0.2:\n",
    "                noise.reset(mean=0, std=0.5)\n",
    "                agent.bstep.obs_noise_range = arg.obs_noise_range\n",
    "                agent.memory.reset()\n",
    "                tot_t = 0\n",
    "                episode = 0\n",
    "                pre_phase = False\n",
    "\n",
    "            # reset logs\n",
    "            reward_log = []\n",
    "            rewarded_trial_log = []\n",
    "            step_log = []\n",
    "            actor_loss_log = 0\n",
    "            critic_loss_log = 0\n",
    "            num_update = 1e-5\n",
    "            dist_log = []\n",
    "\n",
    "        # save checkpoints and validation\n",
    "        if episode % VALIDATION_FREQ == VALIDATION_FREQ - 1:\n",
    "            # save\n",
    "            agent.save(save_memory=False, episode=episode, pre_phase=pre_phase, full_param=False)\n",
    "            # validation for deciding the training phase\n",
    "            if noise.std < init_expnoise_std and decrease_lr:\n",
    "                validator(agent, episode).to_csv(arg.data_path / f'{arg.filename}.csv', index=False)\n",
    "                agent.bstep.obs_noise_range = arg.obs_noise_range\n",
    "\n",
    "        episode += 1\n",
    "        \n",
    "        # break if no learning\n",
    "        if pre_phase and episode >= 5e4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specify parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'gain'    # possible arguments: 'gain'; 'gain_control'.\n",
    "seeds = [0, 1, 2]\n",
    "TOTAL_EPISODE = 1e4   # total training trials; default training: 1e4\n",
    "pro_noise = 0.2       # process noise std, applied when task is 'gain_control'.\n",
    "obs_noise = 0.1       # observation noise std, applied when task is 'gain_control'.\n",
    "extensive = False     # is extensive training?\n",
    "folder_path = Path('../data/agents_temp')  # root folder for all agents' checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    datapath = folder_path / 'EKF' / task / f'seed{seed}'\n",
    "    training(datapath, seed, task, TOTAL_EPISODE, pro_noise, obs_noise, extensive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
