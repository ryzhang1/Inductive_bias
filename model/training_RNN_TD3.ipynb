{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import config.config as config\n",
    "from validation import Validation\n",
    "from Environment import Env\n",
    "from Agent_RNN import *\n",
    "from pathlib import Path\n",
    "\n",
    "import sys; sys.path.append('../analysis/')\n",
    "from my_utils import reset_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(datapath, seed_number, Actor, Critic, task='gain', \n",
    "             init_expnoise_std=0.8, TOTAL_EPISODE=1e5, pro_noise=0.2, obs_noise=0.1, extensive=False):\n",
    "    if task == 'gain':\n",
    "        arg = config.ConfigGain(datapath)\n",
    "    elif task == 'gain_control':\n",
    "        arg = config.ConfigGainControl(datapath, pro_noise=pro_noise, obs_noise=obs_noise)\n",
    "    else:\n",
    "        raise ValueError('No such a task!')\n",
    "            \n",
    "    arg.SEED_NUMBER = seed_number\n",
    "    arg.save()\n",
    "    \n",
    "    # reproducibility\n",
    "    reset_seeds(arg.SEED_NUMBER)\n",
    "\n",
    "    # initialize environment and agent\n",
    "    env = Env(arg)\n",
    "    agent = Agent(arg, Actor, Critic)\n",
    "    validator = Validation(arg.task, agent_name='RNN', extensive=extensive)\n",
    "    \n",
    "    # define exploration noise\n",
    "    noise = ActionNoise(arg.ACTION_DIM, mean=0, std=init_expnoise_std)\n",
    "\n",
    "    # Remove observation noise in the beginning to help learning in the early stage.\n",
    "    agent.bstep.obs_noise_range = None\n",
    "    \n",
    "    # Loop now\n",
    "    tot_t = 0\n",
    "    episode = agent.initial_episode\n",
    "    reward_log = []\n",
    "    rewarded_trial_log = []\n",
    "    step_log = []\n",
    "    actor_loss_log = 0\n",
    "    critic_loss_log = 0\n",
    "    num_update = 1e-5\n",
    "    dist_log = []\n",
    "\n",
    "    LOG_FREQ = 100\n",
    "    VALIDATION_FREQ = 500\n",
    "    decrease_lr = True\n",
    "    REPLAY_PERIOD = 4    # critic update frequency\n",
    "    PRE_LEARN_PERIOD = arg.BATCH_SIZE * 50     # no learning in the first n trials\n",
    "\n",
    "    enable_mirror_traj = True     # store mirror image of each trajectory\n",
    "    pre_phase=True                # training phase I\n",
    "\n",
    "    # Start loop\n",
    "    while episode < TOTAL_EPISODE:\n",
    "        # initialize a trial\n",
    "        cross_start_threshold = False\n",
    "        reward = torch.zeros(1, 1, 1)\n",
    "\n",
    "        x = env.reset()\n",
    "        agent.bstep.reset(env.pro_gains)\n",
    "        last_action = torch.zeros(1, 1, arg.ACTION_DIM)   # the action with exploration noise\n",
    "        last_action_raw = last_action.clone()  # the exploration noise-free action\n",
    "\n",
    "        # state contains observations of linear and angular velocities, last action (efference copy),\n",
    "        # and target's x and y locations when visible\n",
    "        state = torch.cat([x[-arg.OBS_DIM:].view(1, 1, -1), last_action,\n",
    "                           env.target_position_obs.view(1, 1, -1)], dim=2).to(arg.device)\n",
    "\n",
    "        hiddenin = None  # reset RNN memory\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        for t in range(arg.EPISODE_LEN):\n",
    "            # 1. Check if the agent's action crosses the start threshold\n",
    "            if not cross_start_threshold and (last_action_raw.abs() > arg.TERMINAL_ACTION).any():\n",
    "                cross_start_threshold = True\n",
    "\n",
    "            # 2. Take an action based on current state and previous hidden states of RNN units\n",
    "            action, action_raw, hiddenout = agent.select_action(state, hiddenin, action_noise=noise)\n",
    "\n",
    "            # 3. Update the environment given the agent's action\n",
    "            next_x, reached_target, relative_dist = env(x, action, t)\n",
    "\n",
    "            # 4. Collect new observation and construct the next state\n",
    "            next_ox = agent.bstep(next_x)\n",
    "            next_state = torch.cat([next_ox.view(1, 1, -1), action,\n",
    "                                    env.target_position_obs.view(1, 1, -1)], dim=2).to(arg.device)\n",
    "\n",
    "            # 5. Check if agent stops\n",
    "            is_stop = env.is_stop(x, action)\n",
    "\n",
    "            # 6. Give reward if agent stopped         \n",
    "            if is_stop and cross_start_threshold:\n",
    "                reward = env.return_reward(x, reward_mode='mixed')\n",
    "\n",
    "            # 7. Append data\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # 8. Update timestep\n",
    "            last_action_raw = action_raw\n",
    "            last_action = action\n",
    "            state = next_state\n",
    "            x = next_x\n",
    "            hiddenin = hiddenout\n",
    "            tot_t += 1\n",
    "\n",
    "            # 9. Update model\n",
    "            if len(agent.memory.memory) > PRE_LEARN_PERIOD and tot_t % REPLAY_PERIOD == 0:\n",
    "                actor_loss, critic_loss = agent.learn()\n",
    "                actor_loss_log += actor_loss\n",
    "                critic_loss_log += critic_loss\n",
    "                num_update += 1\n",
    "\n",
    "            # 10. Trial ends if agent stops\n",
    "            if is_stop and cross_start_threshold:\n",
    "                break\n",
    "\n",
    "\n",
    "        # End of a trial, store trajectory into buffer\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions).to(arg.device)\n",
    "        rewards = torch.cat(rewards).to(arg.device)\n",
    "        agent.memory.push(states, actions, rewards) \n",
    "\n",
    "        if enable_mirror_traj and noise.std != init_expnoise_std:\n",
    "            # store mirrored trajectories reflected along the y-axis\n",
    "            agent.memory.push(*agent.mirror_traj(states, actions), rewards) \n",
    "\n",
    "        # Logs\n",
    "        reward_log.append(reward.item())\n",
    "        rewarded_trial_log.append(int(reached_target & is_stop))\n",
    "        step_log.append(t + 1)\n",
    "        dist_log.append(relative_dist.item())\n",
    "\n",
    "        if episode % LOG_FREQ == LOG_FREQ - 1:\n",
    "            print(f\"t: {tot_t}, Ep: {episode}, action std: {noise.std:0.2f}\")\n",
    "            print(f\"mean steps: {np.mean(step_log):0.3f}, \"\n",
    "                  f\"mean reward: {np.mean(reward_log):0.3f}, \"\n",
    "                  f\"rewarded fraction: {np.mean(rewarded_trial_log):0.3f}, \"\n",
    "                  f\"relative distance: {np.mean(dist_log) * arg.LINEAR_SCALE:0.3f}, \"\n",
    "                  f\"obs noise: {agent.bstep.obs_noise_range}, \"\n",
    "                  f\"critic loss: {critic_loss_log / num_update:0.3f}, \"\n",
    "                  f\"actor loss: {-actor_loss_log / (num_update/2):0.3f}\")\n",
    "\n",
    "            # training phase III\n",
    "            if decrease_lr and (validator.data.reward_fraction > 0.8).any():\n",
    "                noise.reset(mean=0, std=0.4)\n",
    "                agent.actor_optim.param_groups[0]['lr'] = arg.decayed_lr\n",
    "                agent.critic_optim.param_groups[0]['lr'] = arg.decayed_lr\n",
    "                decrease_lr = False\n",
    "                print('Noise and learning rate are changed.')\n",
    "\n",
    "            # training phase II\n",
    "            if noise.std == init_expnoise_std and np.mean(rewarded_trial_log) > 0.2:\n",
    "                noise.reset(mean=0, std=0.5)\n",
    "                agent.bstep.obs_noise_range = arg.obs_noise_range\n",
    "                agent.memory.reset()\n",
    "                tot_t = 0\n",
    "                episode = 0\n",
    "                pre_phase = False\n",
    "\n",
    "            # reset logs\n",
    "            reward_log = []\n",
    "            rewarded_trial_log = []\n",
    "            step_log = []\n",
    "            actor_loss_log = 0\n",
    "            critic_loss_log = 0\n",
    "            num_update = 1e-5\n",
    "            dist_log = []\n",
    "\n",
    "        # save checkpoints and validation\n",
    "        if episode % VALIDATION_FREQ == VALIDATION_FREQ - 1:\n",
    "            # save\n",
    "            agent.save(save_memory=False, episode=episode, pre_phase=pre_phase, full_param=False)\n",
    "            # validation for deciding the training phase\n",
    "            if noise.std < init_expnoise_std and decrease_lr:\n",
    "                validator(agent, episode).to_csv(arg.data_path / f'{arg.filename}.csv', index=False)\n",
    "                agent.bstep.obs_noise_range = arg.obs_noise_range\n",
    "\n",
    "        episode += 1\n",
    "        \n",
    "        # break if no learning\n",
    "        if pre_phase and episode >= 5e4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specify parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = ['Actor3']   # possible arguments: 'Actor1'; 'Actor2'; 'Actor3'\n",
    "critics = ['Critic5'] # possible arguments: 'Critic1'; 'Critic2'; 'Critic3'; 'Critic4'; 'Critic5'\n",
    "tasks = ['gain']      # possible arguments: 'gain'; 'gain_control'\n",
    "seeds = [[0, 1, 2]]   # should be an iterable\n",
    "init_expnoise_std = 0.8    # initial exploration noise std\n",
    "TOTAL_EPISODE = 1e4        # total training trials; default training: 1e4, extensive training: 1e5\n",
    "pro_noise = 0.2            # process noise std, applied when task is 'gain_control'. In paper, values 0, 0.1, 0.2, 0.3 are used.\n",
    "obs_noise = 0.1            # observation noise std, applied when task is 'gain_control'. In paper, values 0, 0.1, 0.2, 0.3 are used.\n",
    "extensive = True           # is extensive training?\n",
    "folder_path = Path('../data/agents_temp')  # root folder for all agents' checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for actor, critic, task, seed_ in zip(actors, critics, tasks, seeds):\n",
    "    for seed in seed_:\n",
    "        if task == 'gain_control':\n",
    "            datapath = folder_path / f'{actor}{critic}' / task \\\n",
    "                        / f'{pro_noise}_{obs_noise}' / f'seed{seed}'\n",
    "        else:\n",
    "            datapath = folder_path / f'{actor}{critic}' / task / f'seed{seed}'\n",
    "        exec(f'from {actor} import *'); exec(f'from {critic} import *')\n",
    "        training(datapath, seed, Actor, Critic, task, init_expnoise_std, TOTAL_EPISODE, \n",
    "                 pro_noise, obs_noise, extensive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
